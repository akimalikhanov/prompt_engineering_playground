# config/models.yaml
# Purpose: single registry for router lookups (Stage 3)
# - Maps a simple model_id (what your CLI uses) -> adapter module/function + model name/path
# - Holds safe defaults (stream on, temp/top_p/max_tokens/seed)
# - Includes local vLLM server info

providers:
  openai:
    base_url: "https://api.openai.com/v1/"
    api_key_env: "OPENAI_API_KEY"
  google:
    base_url: "https://generativelanguage.googleapis.com/v1beta/openai/"
    api_key_env: "GOOGLE_API_KEY"
  vllm:
    # base_url is per-model using the server config in each model
    api_key_env: null   # not required; a dummy key will be used

defaults:
  params:
    temperature: 0.7
    top_p: 1.0
    max_tokens: 512
    seed: null
  stream: true

models:
  # === Cloud adapters ===
  - id: gpt-5.2                  # matches your --model_id choices
    label: "OpenAI GPT-5.2"
    provider: openai
    model_name: "gpt-5.2"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: ${GPT5_2_INPUT_PRICE:-0.00175}
      output: ${GPT5_2_OUTPUT_PRICE:-0.01400}
      cached_input: null
      cached_output: null
      rounding: exact
      effective: 2025-12-11
      notes: "OpenAI GPT-5.2 standard-tier pricing (per 1M: $1.75 in, $14 out)."
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {}       # keep empty to use defaults

  - id: gpt-5-mini              # matches your --model_id choices
    label: "OpenAI GPT-5-mini"
    provider: openai
    model_name: "gpt-5-mini"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: ${GPT5_MINI_INPUT_PRICE:-0.00025}
      output: ${GPT5_MINI_OUTPUT_PRICE:-0.00200}
      cached_input: null          # set if OpenAI exposes cheaper cached input
      cached_output: null         # likewise
      rounding: exact             # you can choose ceil_1k / nearest_1k / exact
      effective: 2025-12-11
      notes: "OpenAI GPT-5-mini standard-tier pricing (per 1M: $0.25 in, $2 out)."
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {temperature: 1.0}       # keep empty to use defaults

  - id: gpt-5-nano              # ultra-cheap background / batch jobs
    label: "OpenAI GPT-5-nano"
    provider: openai
    model_name: "gpt-5-nano"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: ${GPT5_NANO_INPUT_PRICE:-0.00005}
      output: ${GPT5_NANO_OUTPUT_PRICE:-0.00040}
      cached_input: null
      cached_output: null
      rounding: exact
      effective: 2025-12-11
      notes: "OpenAI GPT-5-nano standard-tier pricing (per 1M: $0.05 in, $0.40 out)."
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {temperature: 1.0}
    
  - id: gpt-4o-mini              # matches your --model_id choices
    label: "OpenAI GPT-4o-mini"
    provider: openai
    model_name: "gpt-4o-mini"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: ${GPT_MINI_INPUT_PRICE:-0.00015}
      output: ${GPT_MINI_OUTPUT_PRICE:-0.00060}
      cached_input: null          # set if OpenAI exposes cheaper cached input
      cached_output: null         # likewise
      rounding: exact             # you can choose ceil_1k / nearest_1k / exact
      effective: 2025-07-18       # date of pricing announcement
      notes: "OpenAI list price: $0.15 per 1M input tokens, $0.60 per 1M output tokens"
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {}       # keep empty to use defaults

  - id: gpt-o4-mini               # matches your --model_id choices
    label: "OpenAI GPT o4-mini"
    provider: openai
    model_name: "o4-mini"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: ${GPT_O4_MINI_INPUT_PRICE:-0.00110}
      output: ${GPT_O4_MINI_OUTPUT_PRICE:-0.00440}
      cached_input: null
      cached_output: null
      rounding: exact
      effective: 2025-04-16       # release date
      notes: "OpenAI list price: $1.10 per 1M input tokens, $4.40 per 1M output tokens. Compact reasoning model with 200k context window."
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {temperature: 1.0}   # o4-mini only supports temperature=1

  - id: gemini-flash-lite
    label: "Gemini 2.5 Flash Lite"
    provider: google
    model_name: "gemini-2.5-flash-lite"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: ${GEMINI_FLASH_LITE_INPUT_PRICE:-0.0}
      output: ${GEMINI_FLASH_LITE_OUTPUT_PRICE:-0.0}
      notes: "Free tier is used (daily rate limits are applied by Google)"
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {}

  - id: gemini-flash
    label: "Gemini 2.5 Flash"
    provider: google
    model_name: "gemini-2.5-flash"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: ${GEMINI_FLASH_INPUT_PRICE:-0.0}
      output: ${GEMINI_FLASH_OUTPUT_PRICE:-0.0}
      notes: "Free tier is used (daily rate limits are applied by Google)"
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {max_tokens: 2000}

  # - id: gemini-pro
  #   label: "Gemini 2.5 Pro"
  #   provider: google
  #   model_name: "gemini-2.5-pro"
  #   pricing:
  #     currency: USD
  #     unit: per_1k_tokens
  #     input: 0.0
  #     output: 0.0
  #     notes: "Free tier is used (daily rate limits are applied by Google)"
  #   capabilities:
  #     streaming: true
  #   params_override: {max_tokens: 2000}

  # === Local vLLM adapters ===
  - id: llama
    label: "Llama3.1-8B-Instruct-AWQ"
    provider: vllm
    model_path: "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"  # your local HF path or name
    server:
      host: "${LLAMA_SERVER_HOST}"
      port: "${LLAMA_SERVER_PORT:-8002}"           # was vllm_dict['llama']['port']
      base_path: "/v1"                          # router will format http://host:port/base_path
    pricing:
      currency: USD
      unit: local            # indicates no per-token pricing
      cost: 0.0              # treated as free in cost dashboards
      notes: "Running on self-hosted GPU with vLLM. No API billing."
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {}

  - id: qwen
    label: "Qwen2.5-7B-Instruct-AWQ "
    provider: vllm
    model_path: "Qwen/Qwen2.5-7B-Instruct-AWQ"  # your local HF path or name
    server:
      host: "${QWEN_SERVER_HOST}"
      port: "${QWEN_SERVER_PORT:-8001}"           # was vllm_dict['qwen']['port']
      base_path: "/v1"
    pricing:
      currency: USD
      unit: local            # indicates no per-token pricing
      cost: 0.0              # treated as free in cost dashboards
      notes: "Running on self-hosted GPU with vLLM. No API billing."
    capabilities:
      streaming: true
      tool_calling: true
    params_override: {}
