# config/models.yaml
# Purpose: single registry for router lookups (Stage 3)
# - Maps a simple model_id (what your CLI uses) -> adapter module/function + model name/path
# - Holds safe defaults (stream on, temp/top_p/max_tokens/seed)
# - Includes local vLLM server info

providers:
  openai:
    base_url: "https://api.openai.com/v1/"
    api_key_env: "OPENAI_API_KEY"
  google:
    base_url: "https://generativelanguage.googleapis.com/v1beta/openai/"
    api_key_env: "GOOGLE_API_KEY"
  vllm:
    # base_url is per-model using the server config in each model
    api_key_env: null   # not required; a dummy key will be used

defaults:
  params:
    temperature: 0.7
    top_p: 1.0
    max_tokens: 256
    seed: null
  stream: true

models:
  # === Cloud adapters ===
  - id: gpt                   # matches your --model_id choices
    label: "OpenAI GPT-4o"
    provider: openai
    model_name: "gpt-4o"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: 0.00250           # $/1,000 input tokens for GPT-4o
      output: 0.01000          # $/1,000 output tokens
      cached_input: null
      cached_output: null
      rounding: exact
      effective: 2025-05-13    # date when full GPT-4o API price was announced (approx) :contentReference[oaicite:3]{index=3}
      notes: "OpenAI list price for full GPT-4o: $2.50 per 1M input, $10.00 per 1M output tokens"
    capabilities:
      streaming: true
    params_override: {}       # keep empty to use defaults

  - id: gpt-mini              # matches your --model_id choices
    label: "OpenAI GPT-4o-mini"
    provider: openai
    model_name: "gpt-4o-mini"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: 0.00015              # $/1,000 input tokens for GPT-4o-mini
      output: 0.00060             # $/1,000 output tokens
      cached_input: null          # set if OpenAI exposes cheaper cached input
      cached_output: null         # likewise
      rounding: exact             # you can choose ceil_1k / nearest_1k / exact
      effective: 2025-07-18       # date of pricing announcement
      notes: "OpenAI list price: $0.15 per 1M input tokens, $0.60 per 1M output tokens"
    capabilities:
      streaming: true
    params_override: {}       # keep empty to use defaults

  - id: gpt-o4-mini               # matches your --model_id choices
    label: "OpenAI GPT o4-mini"
    provider: openai
    model_name: "o4-mini"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: 0.00110              # $/1,000 input tokens for o4-mini
      output: 0.00440             # $/1,000 output tokens
      cached_input: null
      cached_output: null
      rounding: exact
      effective: 2025-04-16       # release date
      notes: "OpenAI list price: $1.10 per 1M input tokens, $4.40 per 1M output tokens. Compact reasoning model with 200k context window."
    capabilities:
      streaming: true
    params_override: {temperature: 1.0}   # o4-mini only supports temperature=1

  - id: gemini-flash-lite
    label: "Gemini 2.5 Flash Lite"
    provider: google
    model_name: "gemini-2.5-flash-lite"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: 0.0
      output: 0.0
      notes: "Free tier is used (daily rate limits are applied by Google)"
    capabilities:
      streaming: true
    params_override: {}

  - id: gemini-flash
    label: "Gemini 2.5 Flash"
    provider: google
    model_name: "gemini-2.5-flash"
    pricing:
      currency: USD
      unit: per_1k_tokens
      input: 0.0
      output: 0.0
      notes: "Free tier is used (daily rate limits are applied by Google)"
    capabilities:
      streaming: true
    params_override: {max_tokens: 2000}

  # - id: gemini-pro
  #   label: "Gemini 2.5 Pro"
  #   provider: google
  #   model_name: "gemini-2.5-pro"
  #   pricing:
  #     currency: USD
  #     unit: per_1k_tokens
  #     input: 0.0
  #     output: 0.0
  #     notes: "Free tier is used (daily rate limits are applied by Google)"
  #   capabilities:
  #     streaming: true
  #   params_override: {max_tokens: 2000}

  # === Local vLLM adapters ===
  - id: llama
    label: "Llama3.1-8B-Instruct-AWQ"
    provider: vllm
    model_path: "./storage/llama3.1-8b-awq"  # your local HF path or name
    server:
      host: "0.0.0.0"
      port: 8002                                # was vllm_dict['llama']['port']
      base_path: "/v1"                          # router will format http://host:port/base_path
    pricing:
      currency: USD
      unit: local            # indicates no per-token pricing
      cost: 0.0              # treated as free in cost dashboards
      notes: "Running on self-hosted GPU with vLLM. No API billing."
    capabilities:
      streaming: true
    params_override: {}

  - id: qwen
    label: "Qwen2.5-7B-Instruct-AWQ "
    provider: vllm
    model_path: "Qwen/Qwen2.5-7B-Instruct-AWQ"  # your local HF path or name
    server:
      host: "0.0.0.0"
      port: 8001                                # was vllm_dict['qwen']['port']
      base_path: "/v1"
    pricing:
      currency: USD
      unit: local            # indicates no per-token pricing
      cost: 0.0              # treated as free in cost dashboards
      notes: "Running on self-hosted GPU with vLLM. No API billing."
    capabilities:
      streaming: true
    params_override: {}
