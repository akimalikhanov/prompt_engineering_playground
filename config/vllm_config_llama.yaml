# config.yaml

model: "./storage/llama3.1-8b-awq"
host: "0.0.0.0"
port: 8002
max_model_len: 4096
max_num_seqs: 1
max_num_batched_tokens: 4096 # â‰¤ max_num_seqs * max_model_len
gpu_memory_utilization: 0.7
quantization: awq_marlin
dtype: half
uvicorn_log_level: "info"