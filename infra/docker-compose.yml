services:
  postgres:
    image: postgres:15-alpine
    container_name: mlflow-postgres
    # Use CLI --env-file so variable interpolation works from ../.env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}

      # Passed through so the init script can read them
      MLFLOW_DB: ${MLFLOW_DB}
      MLFLOW_DB_USER: ${MLFLOW_DB_USER}
      MLFLOW_DB_PASSWORD: ${MLFLOW_DB_PASSWORD}
      APP_DB: ${APP_DB}
      APP_DB_USER: ${APP_DB_USER}
      APP_DB_PASSWORD: ${APP_DB_PASSWORD}
    volumes:
      - ../storage/postgres:/var/lib/postgresql/data
      - ./db_init:/docker-entrypoint-initdb.d
    ports:
      - "${POSTGRES_PORT:-5433}:5432"
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 3s
      retries: 30
    restart: unless-stopped

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    container_name: mlflow-minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - ../storage/minio:/data
    ports:
      - "${MINIO_API_PORT:-9000}:9000"   # S3 API
      - "${MINIO_CONSOLE_PORT:-9001}:9001"   # Web Console
    restart: unless-stopped

  minio-init:
    image: minio/mc:RELEASE.2025-08-13T08-35-41Z
    container_name: mlflow-minio-init
    depends_on:
      - minio
    env_file: ../.env
    volumes:
      - ./minio_init:/scripts
    entrypoint: ["/bin/sh", "/scripts/00_minio_init.sh"]
    restart: "no"

  mlflow:
    build:
      context: ..
      dockerfile: infra/docker_images/Dockerfile.mlflow
    container_name: mlflow-server
    depends_on:
      postgres:
        condition: service_healthy
      pgbouncer:
        condition: service_started
      minio:
        condition: service_started
    env_file: ../.env
    environment:
      # S3/MinIO config (needed by MLflow to write artifacts)
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: us-east-1
      # Allow API/other containers to reach MLflow without host-header 403s
      # For tighter security, replace '*' with an explicit comma-separated allowlist
      MLFLOW_SERVER_ALLOWED_HOSTS: "*"
    command:
      - sh
      - -lc
      - >
        mlflow server
        --host 0.0.0.0
        --port ${MLFLOW_PORT}
        --backend-store-uri postgresql+psycopg2://${MLFLOW_DB_USER}:${MLFLOW_DB_PASSWORD}@pgbouncer:${PGB_APP_DB_PORT}/${MLFLOW_DB}
        --artifacts-destination s3://${MINIO_BUCKET}
        --serve-artifacts
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    restart: unless-stopped

  api:
    build:
      context: ..
      dockerfile: infra/docker_images/Dockerfile.base
      target: api
    container_name: api-server
    depends_on:
      postgres:
        condition: service_healthy
      pgbouncer:
        condition: service_started
      mlflow:
        condition: service_started
    env_file: ../.env
    extra_hosts: ["host.docker.internal:host-gateway"]
    ports:
      - "${API_PORT}:8000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ui:
    build:
      context: ..
      dockerfile: infra/docker_images/Dockerfile.base
      target: ui
    container_name: ui-server
    depends_on:
      api:
        condition: service_started
    env_file: ../.env
    ports:
      - "${UI_PORT}:7860"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.140.1
    container_name: otel-collector
    command: ["--config=/etc/otelcol/otel-collector.yaml"]
    volumes:
      - ./monitoring/otel-collector.yaml:/etc/otelcol/otel-collector.yaml:ro
    depends_on:
      - tempo
      - loki
    ports:
      - "${OTEL_HTTP_PORT:-4318}:4318"     # OTLP/HTTP
      - "${OTEL_HEALTH_PORT:-13133}:13133"   # health check
      - "${OTEL_PROM_METRICS_PORT:-9464}:9464" # Prometheus scrape endpoint
    restart: unless-stopped
    # Note: Healthcheck removed as this minimal image lacks shell/curl/wget
    # Service health can be verified via: curl http://localhost:13133/

  tempo:
    image: grafana/tempo:2.8.2
    container_name: tempo
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    user: "0"  # run as root to ensure write access to named volume (/tmp/tempo)
    depends_on:
      - minio
    env_file: ../.env
    environment:
      # S3/MinIO config for storing traces
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: us-east-1
    volumes:
      - ./monitoring/tempo.yaml:/etc/tempo/tempo.yaml:ro
      - tempo-data:/tmp/tempo    # persistent local storage for WAL
    ports:
      - "${TEMPO_HTTP_PORT:-3200}:3200"   # Tempo HTTP API
      - "${TEMPO_GRPC_PORT:-4317}:4317"   # OTLP/gRPC (from collector)
      - "${TEMPO_OTLP_HTTP_PORT:-4319}:4318"   # OTLP/HTTP (optional external)
      - "${TEMPO_MEMBERLIST_PORT:-7946}:7946"   # memberlist port
    restart: unless-stopped

  loki:
    image: grafana/loki:3.5.7
    container_name: loki
    command:
      - -config.file=/etc/loki/loki-config.yaml
      - -config.expand-env=true
    env_file: ../.env
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
      LOKI_BUCKET: ${LOKI_BUCKET:-loki-logs}
      LOKI_S3_ENDPOINT: ${LOKI_S3_ENDPOINT:-minio:9000}
    depends_on:
      minio:
        condition: service_started
    user: "0"  # run as root to ensure write access to named volume (see loki-config paths)
    volumes:
      - ./monitoring/loki-config.yaml:/etc/loki/loki-config.yaml:ro
      - loki-data:/tmp/loki   # named volume for WAL, cache, compactor data (matches loki-config.yaml)
    ports:
      - "${LOKI_HTTP_PORT:-3100}:3100"   # Loki HTTP API
    restart: unless-stopped

  grafana:
    image: grafana/grafana:12.2.0
    container_name: grafana
    env_file:
      - ../.env
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASS}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_MAX_REQUEST_SIZE=50MB
      - GF_SERVER_MAX_QUERY_RESULT_SIZE=50MB
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "${GRAFANA_PORT:-3000}:3000"   # UI â†’ http://localhost:3000
    restart: unless-stopped
    depends_on:
      - tempo
      - loki

  prometheus:
    image: prom/prometheus:v3.7.3
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus   # persistent TSDB storage
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.enable-admin-api
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    restart: unless-stopped
    depends_on:
      - otel-collector

  pgbouncer:
    image: edoburu/pgbouncer:v1.24.1-p1
    container_name: pgbouncer
    depends_on:
      - postgres
    ports:
      - "6432:6432"
    env_file:
      - ../.env
    entrypoint: ["/bin/sh", "/pgbouncer-entrypoint.sh"]
    volumes:
      - ./shell_scripts/pgbouncer-entrypoint.sh:/pgbouncer-entrypoint.sh:ro
    restart: unless-stopped


volumes:
  tempo-data:
  grafana-data:
  loki-data:
  prometheus-data:
