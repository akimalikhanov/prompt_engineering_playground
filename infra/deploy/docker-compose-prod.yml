name: pep

services:
  postgres:
    image: postgres:15-alpine
    container_name: app-postgres
    # Use CLI --env-file so variable interpolation works from ../.env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      # Passed through so the init script can read them
      MLFLOW_DB: ${MLFLOW_DB}
      MLFLOW_DB_USER: ${MLFLOW_DB_USER}
      MLFLOW_DB_PASSWORD: ${MLFLOW_DB_PASSWORD}
      APP_DB: ${APP_DB}
      APP_DB_USER: ${APP_DB_USER}
      APP_DB_PASSWORD: ${APP_DB_PASSWORD}
    volumes:
      - /opt/pep/data/postgres:/var/lib/postgresql/data
      - ../db_init:/docker-entrypoint-initdb.d
    ports:
      - "127.0.0.1:${POSTGRES_PORT:-5433}:5432"
    networks:
      - public
      - internal
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 30
    mem_limit: 2G
    mem_reservation: 512M
    cpus: '1.5'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 6m
    restart: unless-stopped

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    container_name: app-minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      # Used by healthcheck (MinIO always listens on 9000 inside container)
      MINIO_API_PORT: ${MINIO_API_PORT:-9000}
    command: server /data --console-address ":9001"
    volumes:
      - /opt/pep/data/minio:/data
    ports:
      - "127.0.0.1:${MINIO_API_PORT:-9000}:9000"   # S3 API
      - "127.0.0.1:${MINIO_CONSOLE_PORT:-9001}:9001"   # Web Console
    networks:
      - public
      - internal
    mem_limit: 1G
    mem_reservation: 256M
    cpus: '1.0'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 8m
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:$$MINIO_API_PORT/minio/health/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  minio-init:
    image: minio/mc:RELEASE.2025-08-13T08-35-41Z
    container_name: app-minio-init
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      # Main bucket for MLflow artifacts
      MINIO_BUCKET: ${MINIO_BUCKET}
      # Optional buckets created by init script (defaults are in the script)
      LOKI_BUCKET: ${LOKI_BUCKET:-loki-logs}
      # Script expects TEMPO_BUCKET; .env.example uses TEMPO_BUCKET_NAME
      TEMPO_BUCKET: ${TEMPO_BUCKET_NAME:-tempo-traces}
    volumes:
      - ../minio_init:/scripts
    entrypoint: ["/bin/sh", "/scripts/00_minio_init.sh"]
    networks:
      - internal
    restart: "no"

  mlflow:
    image: ${ECR_REGISTRY}/${ECR_REPO_MLFLOW}:${IMAGE_TAG}
    build:
      context: ../..
      dockerfile: infra/docker_images/Dockerfile.mlflow
    container_name: mlflow-server
    depends_on:
      pgbouncer:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      # S3/MinIO config (needed by MLflow to write artifacts)
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: us-east-1
      # Backend DB config (used in command via $$... runtime expansion)
      MLFLOW_DB: ${MLFLOW_DB}
      MLFLOW_DB_USER: ${MLFLOW_DB_USER}
      MLFLOW_DB_PASSWORD: ${MLFLOW_DB_PASSWORD}
      PGB_APP_DB_PORT: ${PGB_APP_DB_PORT:-6432}
      # Artifacts bucket (used in command via $$... runtime expansion)
      MINIO_BUCKET: ${MINIO_BUCKET}
      # Used by healthcheck
      MLFLOW_PORT: ${MLFLOW_PORT}
      # Allow API/other containers to reach MLflow without host-header 403s
      # Tighten Host-header validation (avoid accepting arbitrary hosts)
      # Include both Docker DNS names and local dev access via port-mapped localhost.
      # If you access MLflow via a real DNS name (e.g. mlflow.example.com), add it here.
      # For EC2 behind nginx, set MLFLOW_PUBLIC_HOST to your public hostname (e.g. ec2-xx-xx-xx-xx.<region>.compute.amazonaws.com).
      MLFLOW_PUBLIC_HOST: ${MLFLOW_PUBLIC_HOST:-}
      MLFLOW_SERVER_ALLOWED_HOSTS: "mlflow,mlflow:${MLFLOW_PORT},mlflow-server,mlflow-server:${MLFLOW_PORT},localhost,localhost:${MLFLOW_PORT},127.0.0.1,127.0.0.1:${MLFLOW_PORT},${MLFLOW_PUBLIC_HOST},${MLFLOW_PUBLIC_HOST}:${MLFLOW_PORT}"
    command:
      - sh
      - -lc
      - >
        mlflow server
        --host 0.0.0.0
        --port "$$MLFLOW_PORT"
        --backend-store-uri "postgresql+psycopg2://$$MLFLOW_DB_USER:$$MLFLOW_DB_PASSWORD@pgbouncer:$$PGB_APP_DB_PORT/$$MLFLOW_DB"
        --artifacts-destination "s3://$$MINIO_BUCKET"
        --serve-artifacts
    ports:
      - "127.0.0.1:${MLFLOW_PORT:-5000}:${MLFLOW_PORT:-5000}"
    networks:
      - public
      - internal
    mem_limit: 2.5G
    mem_reservation: 512M
    cpus: '1.0'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 12m
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:$$MLFLOW_PORT/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  api:
    image: ${ECR_REGISTRY}/${ECR_REPO_API}:${IMAGE_TAG}
    build:
      context: ../..
      dockerfile: infra/docker_images/Dockerfile.base
      target: api
    container_name: api-server
    depends_on:
      pgbouncer:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    environment:
      # MLflow tracking (used by api/main.py)
      MLFLOW_HOST: ${MLFLOW_HOST:-mlflow}
      MLFLOW_PORT: ${MLFLOW_PORT:-5000}

      # App DB (used by services/prompts_service.py and services/runs_logger.py)
      PGB_APP_DB_HOST: ${PGB_APP_DB_HOST:-pgbouncer}
      PGB_APP_DB_PORT: ${PGB_APP_DB_PORT:-6432}
      APP_DB: ${APP_DB:-app}
      APP_DB_USER: ${APP_DB_USER:-app_user}
      APP_DB_PASSWORD: ${APP_DB_PASSWORD}

      # Logging / OTEL
      LOGGER_NAME: ${LOGGER_NAME:-llm-router}
      OTEL_LOGS_ENABLED: ${OTEL_LOGS_ENABLED:-True}
      OTEL_SERVICE_NAME: ${OTEL_SERVICE_NAME:-pep-api}
      OTEL_SERVICE_NAMESPACE: ${OTEL_SERVICE_NAMESPACE:-pep}
      OTEL_DEPLOYMENT_ENVIRONMENT: ${OTEL_DEPLOYMENT_ENVIRONMENT:-prod}
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4318}

      # Provider keys used by router (config/models.yaml)
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY:-}

      # Local vLLM endpoints referenced in config/models.yaml
      # If vLLM containers are on the same Docker network, use container names:
      #   LLAMA_SERVER_HOST=vllm-llama  QWEN_SERVER_HOST=vllm-qwen
      # If vLLM is on a different EC2, use that EC2's private IP:
      #   LLAMA_SERVER_HOST=10.0.2.123  QWEN_SERVER_HOST=10.0.2.123
      LLAMA_SERVER_HOST: ${LLAMA_SERVER_HOST:-vllm-llama}
      LLAMA_SERVER_PORT: ${LLAMA_SERVER_PORT:-8002}
      QWEN_SERVER_HOST: ${QWEN_SERVER_HOST:-vllm-qwen}
      QWEN_SERVER_PORT: ${QWEN_SERVER_PORT:-8001}

      # Tool APIs (used by services/llm_tools.py when tools are enabled)
      OMDB_API_KEY: ${OMDB_API_KEY:-}
      FMP_API_KEY: ${FMP_API_KEY:-}
      FMP_BASE_URL: ${FMP_BASE_URL:-https://financialmodelingprep.com/stable}

      # Used by healthcheck (API listens on 8000 inside container)
      API_PORT: ${API_PORT:-8000}
    extra_hosts: ["host.docker.internal:host-gateway"]
    ports:
      - "127.0.0.1:${API_PORT:-8000}:8000"
    networks:
      - public
      - internal
    mem_limit: 2G
    mem_reservation: 512M
    cpus: '2.0'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 16m
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:$$API_PORT/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ui:
    image: ${ECR_REGISTRY}/${ECR_REPO_UI}:${IMAGE_TAG}
    build:
      context: ../..
      dockerfile: infra/docker_images/Dockerfile.base
      target: ui
    container_name: ui-server
    depends_on:
      api:
        condition: service_healthy
    environment:
      # UI talks to the backend via this base URL
      API_BASE_URL: ${API_BASE_URL:-http://api:8000}
      UI_LOGGER_NAME: ${UI_LOGGER_NAME:-ui}
      OTEL_LOGS_ENABLED: ${OTEL_LOGS_ENABLED:-True}
      OTEL_SERVICE_NAME: ${OTEL_UI_SERVICE_NAME:-pep-ui}
      OTEL_SERVICE_NAMESPACE: ${OTEL_SERVICE_NAMESPACE:-pep}
      OTEL_DEPLOYMENT_ENVIRONMENT: ${OTEL_DEPLOYMENT_ENVIRONMENT:-prod}
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4318}
      # Used by healthcheck (UI listens on 7860 inside container)
      UI_PORT: ${UI_PORT:-7860}
    ports:
      - "127.0.0.1:${UI_PORT:-7860}:7860"
    networks:
      - public
      - internal
    mem_limit: 1G
    mem_reservation: 128M
    cpus: '0.5'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 6m
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:$$UI_PORT/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.140.1
    container_name: otel-collector
    command: ["--config=/etc/otelcol/otel-collector.yaml"]
    volumes:
      - ../monitoring/otel-collector.yaml:/etc/otelcol/otel-collector.yaml:ro
    depends_on:
      - tempo
      - loki
    networks:
      - internal
    mem_limit: 512M
    mem_reservation: 128M
    cpus: '0.5'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 4m
    restart: unless-stopped
    # Note: Healthcheck removed as this minimal image lacks shell/curl/wget
    # Service health can be verified via: curl http://localhost:13133/

  tempo-loki-init:
    image: busybox:1.36
    container_name: tempo-loki-init
    user: "0:0"
    command:
      ["sh","-c","mkdir -p /tmp/tempo /tmp/loki && chown -R 10001:10001 /tmp/tempo /tmp/loki"]
    volumes:
      - tempo-data:/tmp/tempo
      - loki-data:/tmp/loki
    restart: "no"
    networks:
      - internal
    mem_limit: 64M
    mem_reservation: 16M
    cpus: '0.1'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 1m

  # Healthcheck for otel-collector
  otel-collector-hc:
    image: busybox:1.36
    container_name: otel-collector-hc
    depends_on:
      otel-collector:
        condition: service_started
    networks:
      - internal
    mem_limit: 64M
    mem_reservation: 16M
    cpus: '0.1'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 1m
    command: ["sh", "-c", "sleep 9999999"]
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://otel-collector:13133/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped

  tempo:
    image: grafana/tempo:2.8.2
    container_name: tempo
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    # user: "0"  # run as root to ensure write access to named volume (/tmp/tempo)
    depends_on:
      minio:
        condition: service_healthy
      tempo-loki-init:
        condition: service_completed_successfully
    environment:
      # S3/MinIO config for storing traces
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: us-east-1
      # Used by healthcheck (tempo http_listen_port in tempo.yaml)
      TEMPO_HTTP_PORT: ${TEMPO_HTTP_PORT:-3200}
    volumes:
      - ../monitoring/tempo.yaml:/etc/tempo/tempo.yaml:ro
      - tempo-data:/tmp/tempo    # persistent local storage for WAL
    networks:
      - internal
    mem_limit: 2G
    mem_reservation: 512M
    cpus: '1.0'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 10m
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "busybox", "sh", "-c", "wget --spider -q http://localhost:$$TEMPO_HTTP_PORT/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  loki:
    image: grafana/loki:3.5.7
    container_name: loki
    command:
      - -config.file=/etc/loki/loki-config.yaml
      - -config.expand-env=true
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
      LOKI_BUCKET: ${LOKI_BUCKET:-loki-logs}
      LOKI_S3_ENDPOINT: ${LOKI_S3_ENDPOINT:-minio:9000}
      # Use a single var name in healthchecks
      LOKI_PORT: ${LOKI_HTTP_PORT:-3100}
    depends_on:
      minio:
        condition: service_healthy
      tempo-loki-init:
        condition: service_completed_successfully
    # user: "0"  # run as root to ensure write access to named volume (see loki-config paths)
    volumes:
      - ../monitoring/loki-config.yaml:/etc/loki/loki-config.yaml:ro
      - loki-data:/tmp/loki   # named volume for WAL, cache, compactor data (matches loki-config.yaml)
    networks:
      - internal
    mem_limit: 1G
    mem_reservation: 256M
    cpus: '1.0'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 8m
    healthcheck:
      test: ["CMD", "busybox", "sh", "-c", "wget --spider -q http://localhost:$$LOKI_PORT/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped

  grafana:
    image: grafana/grafana:12.2.0
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASS}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_MAX_REQUEST_SIZE=50MB
      - GF_SERVER_MAX_QUERY_RESULT_SIZE=50MB
      - GF_SERVER_DOMAIN=${GRAFANA_PUBLIC_HOST}
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GRAFANA_PORT=${GRAFANA_PORT:-3000}
      - APP_DB_USER=${APP_DB_USER}
      - APP_DB_PASSWORD=${APP_DB_PASSWORD}
      - APP_DB=${APP_DB}
    volumes:
      - grafana-data:/var/lib/grafana
      - ../monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ../monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ../monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "127.0.0.1:${GRAFANA_PORT:-3000}:3000"   # UI â†’ http://localhost:3000
    networks:
      - public
      - internal
    mem_limit: 768M
    mem_reservation: 192M
    cpus: '0.5'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 4m
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:$$GRAFANA_PORT/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      tempo:
        condition: service_healthy
      loki:
        condition: service_healthy
      prometheus:
        condition: service_healthy

  prometheus:
    image: prom/prometheus:v3.7.3
    container_name: prometheus
    volumes:
      - ../monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus   # persistent TSDB storage
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.enable-admin-api
    environment:
      PROMETHEUS_PORT: ${PROMETHEUS_PORT:-9090}
    networks:
      - internal
    mem_limit: 2G
    mem_reservation: 512M
    cpus: '1.0'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 6m
    healthcheck:
      test: ["CMD", "busybox", "sh", "-c", "wget --spider -q http://localhost:$$PROMETHEUS_PORT/-/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped
    depends_on:
      otel-collector-hc:
        condition: service_healthy

  pgbouncer:
    image: edoburu/pgbouncer:v1.24.1-p1
    container_name: pgbouncer
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Postgres superuser (used by entrypoint + healthcheck)
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}

      # App DB user (used by entrypoint)
      APP_DB: ${APP_DB}
      APP_DB_USER: ${APP_DB_USER}
      APP_DB_PASSWORD: ${APP_DB_PASSWORD}

      # MLflow DB user (used by entrypoint)
      MLFLOW_DB: ${MLFLOW_DB}
      MLFLOW_DB_USER: ${MLFLOW_DB_USER}
      MLFLOW_DB_PASSWORD: ${MLFLOW_DB_PASSWORD}

      # Used by healthcheck (connect through pgbouncer itself)
      PGB_APP_DB_HOST: ${PGB_APP_DB_HOST:-pgbouncer}
      PGB_APP_DB_PORT: ${PGB_APP_DB_PORT:-6432}
    entrypoint: ["/bin/sh", "/pgbouncer-entrypoint.sh"]
    volumes:
      - ../shell_scripts/pgbouncer-entrypoint.sh:/pgbouncer-entrypoint.sh:ro
    networks:
      - internal
    mem_limit: 128M
    mem_reservation: 32M
    cpus: '0.25'
    logging:
      driver: local
      options:
        mode: non-blocking
        max-buffer-size: 2m
    healthcheck:
      test:
        [
        "CMD-SHELL",
        "PGPASSWORD=$$POSTGRES_PASSWORD psql -h $$PGB_APP_DB_HOST -p $$PGB_APP_DB_PORT -U $$POSTGRES_USER -d $$POSTGRES_DB -c 'SELECT 1' >/dev/null"
        ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    restart: unless-stopped


networks:
  public:
    driver: bridge
  internal:
    driver: bridge
    internal: true

volumes:
  tempo-data:
  grafana-data:
  loki-data:
  prometheus-data:
